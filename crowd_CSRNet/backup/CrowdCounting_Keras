{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CrowdCounting_Keras","provenance":[],"collapsed_sections":["EsEN9snSRwcY","7metCmPDc8ZW","JnIoHGXPdHe8","sysi4uqtcyvt"],"machine_shape":"hm","mount_file_id":"1pYiZPwzW72fkjEb-ZUZ5BswfflxFP0rl","authorship_tag":"ABX9TyN7cEPdcybdPhn3G6fGlbFf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# "],"metadata":{"id":"YP9d--hV2Z2r"}},{"cell_type":"markdown","source":["# 라이브러리"],"metadata":{"id":"6mxnUgzxsS01"}},{"cell_type":"markdown","source":["## install"],"metadata":{"id":"PQW4lWIpDqxW"}},{"cell_type":"code","source":[""],"metadata":{"id":"7OZU6Lu-DpC_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## import"],"metadata":{"id":"SIjJwkNaDtnq"}},{"cell_type":"code","source":["# Model pack\n","from keras.layers import BatchNormalization\n","from keras.preprocessing.image import load_img,img_to_array\n","from sklearn.metrics import mean_squared_error\n","from keras.initializers import RandomNormal\n","from keras.applications.vgg16 import VGG16\n","from tensorflow.keras.optimizers import SGD\n","from keras.models import Model,Sequential\n","from keras.layers import *\n","from keras import backend as K\n","from keras.models import model_from_json\n","from matplotlib import cm as CM\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tqdm import tqdm\n","import scipy.io as io\n","from PIL import Image\n","import PIL\n","import h5py\n","import os\n","import glob\n","import cv2\n","import random\n","import math\n","import sys\n","\n","\n","# Analysis\n","# from keras.models import model_from_json\n","import pandas as pd\n","from sklearn.metrics import mean_absolute_error\n","import numpy as np\n","\n","\n","# Interface\n","from matplotlib import cm as c\n","\n","\n","# Preprocess\n","from scipy.ndimage.filters import gaussian_filter\n","import scipy\n","import json\n","from matplotlib import cm as CM"],"metadata":{"id":"qf8hdT40BTdX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Set\n",": 데이터 세트는 A와 B의 두 부분으로 나뉩니다.\n","- A 부분은 군중 밀도가 높은 이미지로 구성됩니다.\n","- B 드문드문 군중 장면이 있는 이미지로 구성"],"metadata":{"id":"1wm1yliP6jqw"}},{"cell_type":"markdown","source":["## 데이터 전처리"],"metadata":{"id":"EsEN9snSRwcY"}},{"cell_type":"code","source":["def gaussian_filter_density(gt):\n","    #Generates a density map using Gaussian filter transformation\n","    \n","    density = np.zeros(gt.shape, dtype=np.float32)\n","    \n","    gt_count = np.count_nonzero(gt)\n","    \n","    if gt_count == 0:\n","        return density\n","\n","    # FInd out the K nearest neighbours using a KDTree\n","    \n","    pts = np.array(list(zip(np.nonzero(gt)[1].ravel(), np.nonzero(gt)[0].ravel())))\n","    leafsize = 2048\n","    \n","    # build kdtree\n","    tree = scipy.spatial.KDTree(pts.copy(), leafsize=leafsize)\n","    \n","    # query kdtree\n","    distances, locations = tree.query(pts, k=4)\n","\n","        \n","    for i, pt in enumerate(pts):\n","        pt2d = np.zeros(gt.shape, dtype=np.float32)\n","        pt2d[pt[1],pt[0]] = 1.\n","        if gt_count > 1:\n","            sigma = (distances[i][1]+distances[i][2]+distances[i][3])*0.1\n","        else:\n","            sigma = np.average(np.array(gt.shape))/2./2. #case: 1 point\n","        \n","        #Convolve with the gaussian filter\n","        \n","        density += scipy.ndimage.filters.gaussian_filter(pt2d, sigma, mode='constant')\n","    \n","    return density"],"metadata":{"id":"MJ7AAaj5RtOs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["root = '/content/drive/MyDrive/Final_P_Counting'\n","\n","part_A_train = os.path.join(root,'data/part_A_final/train_data','images')\n","part_A_test = os.path.join(root,'data/part_A_final/test_data','images')\n","part_B_train = os.path.join(root,'data/part_B_final/train_data','images')\n","part_B_test = os.path.join(root,'data/part_B_final/test_data','images')\n","path_sets = [part_A_train,part_A_test,part_B_train,part_B_test]"],"metadata":{"id":"xq5rouX5R_lN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# List of all image paths\n","\n","img_paths = []\n","for path in path_sets:\n","    for img_path in glob.glob(os.path.join(path, '*.jpg')):\n","        img_paths.append(img_path)\n","print(len(img_paths))"],"metadata":{"id":"6U5xYqq1SE4c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(img_paths[299])"],"metadata":{"id":"6rozx1rjRgyl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 정답(밀도) 사진 생성\n","\n","from tqdm import tqdm\n","\n","i = 0\n","for img_path in tqdm(img_paths):\n","\n","    # Load sparse matrix\n","    mat = io.loadmat(img_path.replace('.jpg','.mat').replace('images','ground_truth').replace('IMG_','GT_IMG_'))\n","\n","    #Read image\n","    img= plt.imread(img_path)\n","    \n","    # Create a zero matrix of image size\n","    k = np.zeros((img.shape[0],img.shape[1]))\n","    \n","    gt = mat[\"image_info\"][0,0][0,0][0]\n","    \n","    #Generate hot encoded matrix of sparse matrix\n","    for i in range(0,len(gt)):\n","        if int(gt[i][1])<img.shape[0] and int(gt[i][0])<img.shape[1]:\n","            k[int(gt[i][1]),int(gt[i][0])]=1\n","    \n","    # generate density map\n","    k = gaussian_filter_density(k)\n","    \n","    # File path to save density map\n","    file_path = img_path.replace('.jpg','.h5').replace('images','ground')\n","    \n","    if os.path.exists(file_path):\n","        continue\n","    else:\n","        with h5py.File(file_path, 'w') as hf:\n","                hf['density'] = k\n","\n","    "],"metadata":{"id":"2Jps53_PSFx2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_path = img_paths[22].replace('.jpg','.h5').replace('images','ground') \n","print(file_path)"],"metadata":{"id":"BCA4_zxBSJP2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Sample Ground Truth\n","gt_file = h5py.File(file_path,'r')\n","groundtruth = np.asarray(gt_file['density'])\n","plt.imshow(groundtruth,cmap=CM.jet)\n","print(\"Sum = \" ,np.sum(groundtruth))"],"metadata":{"id":"3LnkbjAJSLYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Image corresponding to the ground truth\n","img = Image.open(file_path.replace('.h5','.jpg').replace('ground','images'))\n","plt.imshow(img)\n","print(file_path.replace('.h5','.jpg').replace('ground','images'))"],"metadata":{"id":"opjoDRFrSU1R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"rbMLipB39E1T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model\n",": Convolutional Neural Networks를 사용하여 입력 이미지를 해당 밀도 맵에 매핑"],"metadata":{"id":"r1oMkC3V8Sea"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vXFjos_pVQkP"},"outputs":[],"source":["root = '/content/drive/MyDrive/Final_P_Counting'\n","\n","part_A_train = os.path.join(root,'data/part_A_final/train_data','images')\n","part_B_train = os.path.join(root,'data/part_B_final/train_data','images')\n","temp = 'test_images'\n","path_sets_Atrain = [part_A_train]\n","path_sets_Btrain = [part_B_train]\n","\n","display(path_sets_Atrain, path_sets_Btrain) # 입력 확인"]},{"cell_type":"code","source":["img_paths = []\n","\n","for path_A, path_B in zip(path_sets_Atrain, path_sets_Btrain):\n","    \n","    for img_path_A in glob.glob(os.path.join(path_A, '*.jpg')):\n","        img_paths.append(str(img_path_A))\n","    \n","    for img_path_B in glob.glob(os.path.join(path_B, '*.jpg')):\n","        img_paths.append(str(img_path_B))\n","        \n","print(\"Total images : \",len(img_paths))\n","\n","print(img_paths)"],"metadata":{"id":"Sn-ZrBddN6R2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_img(path):\n","    #Function to load,normalize and return image \n","    im = Image.open(path).convert('RGB')\n","    \n","    im = np.array(im)\n","    \n","    im = im/255.0\n","    \n","    im[:,:,0]=(im[:,:,0]-0.485)/0.229\n","    im[:,:,1]=(im[:,:,1]-0.456)/0.224\n","    im[:,:,2]=(im[:,:,2]-0.406)/0.225\n","\n","    #print(im.shape)\n","    #im = np.expand_dims(im,axis  = 0)\n","    return im\n","\n","def get_input(path):\n","    path = path[0] \n","    img = create_img(path)\n","    return img\n","    \n","    \n","    \n","def get_output(path):\n","    #import target\n","    #resize target\n","    \n","    gt_file = h5py.File(path,'r')\n","    \n","    target = np.asarray(gt_file['density'])\n","\n","    # print(target.shape)\n","    \n","    img = cv2.resize(target,(int(target.shape[1]/8),int(target.shape[0]/8)),interpolation = cv2.INTER_CUBIC)*64\n","    \n","    img = np.expand_dims(img, axis = 2) # 출력 shape(x/8, y/8, 1) 형태\n","    \n","    # print(img.shape)\n","    \n","    return img\n","    \n","    \n","    \n","def preprocess_input(image,target):\n","    #crop image\n","    #crop target\n","    #resize target\n","    crop_size = (int(image.shape[0]/2),int(image.shape[1]/2))\n","    \n","    \n","    if random.randint(0,9)<= -1:            \n","            dx = int(random.randint(0,1)*image.shape[0]*1./2)\n","            dy = int(random.randint(0,1)*image.shape[1]*1./2)\n","    else:\n","            dx = int(random.random()*image.shape[0]*1./2)\n","            dy = int(random.random()*image.shape[1]*1./2)\n","\n","    #print(crop_size , dx , dy)\n","    img = image[dx : crop_size[0]+dx , dy:crop_size[1]+dy]\n","    \n","    target_aug = target[dx:crop_size[0]+dx,dy:crop_size[1]+dy]\n","    #print(img.shape)\n","\n","    return(img,target_aug)"],"metadata":{"id":"gpHctEQLPv6a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from numpy.ma.core import outerproduct\n","#Image data generator \n","def image_generator(files, batch_size = 64):\n","    \n","    while True:\n","        # 임의 표본 추출 (a의 리스트에서 size의 개수 만큼 랜덤 추출)\n","        input_path = np.random.choice(a = files, size = batch_size)\n","        \n","        batch_input = []\n","        batch_output = [] \n","          \n","        #for input_path in batch_paths:\n","        \n","        inputt = get_input(input_path ) # 0번째 리스트 inputt\n","        output = get_output(input_path[0].replace('.jpg','.h5').replace('images','ground') )\n","\n","        # print(type(inputt), type(output))\n","       \n","        batch_input += [inputt]\n","        batch_output += [output]\n","    \n","\n","        batch_x = np.array( batch_input )\n","        batch_y = np.array( batch_output )\n","        \n","        yield( batch_x, batch_y ) # 제너레이터 반환"],"metadata":{"id":"YMiiEgKMPyIK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_mod(model , str1 , str2):\n","    model.save_weights(str1)\n","    \n","    model_json = model.to_json()\n","    \n","    with open(str2, \"w\") as json_file:\n","        json_file.write(model_json)"],"metadata":{"id":"tBYzzOk6P0C_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def init_weights_vgg(model):\n","    #vgg =  VGG16(weights='imagenet', include_top=False)\n","    \n","    json_file = open('/content/drive/MyDrive/Final_P_Counting/models/VGG_16.json', 'r')\n","    loaded_model_json = json_file.read()\n","    json_file.close()\n","    loaded_model = model_from_json(loaded_model_json)\n","    loaded_model.load_weights(\"/content/drive/MyDrive/Final_P_Counting/weights/VGG_16.h5\")\n","    \n","    vgg = loaded_model\n","    \n","    vgg_weights=[]                         \n","    for layer in vgg.layers:\n","        if('conv' in layer.name):\n","            vgg_weights.append(layer.get_weights())\n","    \n","    \n","    offset=0\n","    i=0\n","    while(i<10):\n","        if('conv' in model.layers[i+offset].name):\n","            model.layers[i+offset].set_weights(vgg_weights[i])\n","            i=i+1\n","            #print('h')\n","            \n","        else:\n","            offset=offset+1\n","\n","    return (model)"],"metadata":{"id":"iSa9S4QLP1lw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def euclidean_distance_loss(y_true, y_pred):\n","    # Euclidean distance as a measure of loss (Loss function) \n","    return K.sqrt(K.sum(K.square(y_pred - y_true), axis = -1))"],"metadata":{"id":"eTsnEBmeP3-A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Neural network model : VGG + Conv\n","def CrowdNet():  \n","    #Variable Input Size\n","    rows = None\n","    cols = None\n","    \n","    #Batch Normalisation option\n","    \n","    batch_norm = 0\n","    kernel = (3, 3)\n","    init = RandomNormal(stddev=0.01)\n","    model = Sequential() \n","    \n","    #custom VGG:\n","    \n","    if(batch_norm):\n","        model.add(Conv2D(64, kernel_size = kernel, input_shape = (rows,cols,3), activation = 'relu', padding='same'))\n","        model.add(BatchNormalization()) # 배치 정규화는 층으로 들어가는 입력값이 한쪽으로 쏠리거나 너무 퍼지거나 너무 좁아지지 않게 해주는 인공신경망 기법\n","        model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling2D(strides=2))\n","        model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling2D(strides=2))\n","        model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(MaxPooling2D(strides=2))            \n","        model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n","        model.add(BatchNormalization())\n","        model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n","        model.add(BatchNormalization())\n","        \n","    else:\n","        model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same',input_shape = (rows, cols, 3), kernel_initializer = init))\n","        model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n","        model.add(MaxPooling2D(strides=2))\n","        model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n","        model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n","        model.add(MaxPooling2D(strides=2))\n","        model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n","        model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n","        model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n","        model.add(MaxPooling2D(strides=2))            \n","        model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n","        model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n","        model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n","        \n","        \n","\n","        \n","    #Conv2D\n","    model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n","    model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n","    model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n","    model.add(Conv2D(256, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n","    model.add(Conv2D(128, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n","    model.add(Conv2D(64, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n","    model.add(Conv2D(1, (1, 1), activation='relu', dilation_rate = 1, kernel_initializer = init, padding = 'same'))\n","\n","    sgd = SGD(lr = 1e-7, decay = (5*1e-4), momentum = 0.95)\n","    model.compile(optimizer=sgd, loss=euclidean_distance_loss, metrics=['mse'])\n","    \n","    model = init_weights_vgg(model)\n","    \n","    return model"],"metadata":{"id":"v8RpbVvsP5jn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = CrowdNet()\n","model.summary()"],"metadata":{"id":"WJCQW8P2P9Lp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_gen = image_generator(img_paths,1)"],"metadata":{"id":"aNhCC-1CQBjW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sgd = SGD(lr = 1e-7, decay = (5*1e-4), momentum = 0.95)\n","model.compile(optimizer=sgd, loss=euclidean_distance_loss, metrics=['mse'])"],"metadata":{"id":"aOw6uZOxQCoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.fit_generator(train_gen, epochs=1, steps_per_epoch= 300, verbose=1)"],"metadata":{"id":"uknSngNHQOe2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_mod(model,\"/content/drive/MyDrive/Final_P_Counting/weights/model_AB_weights_1.h5\",\"/content/drive/MyDrive/Final_P_Counting/models/Model_AB_1.json\")"],"metadata":{"id":"lOyNl3nKQP00"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TEST"],"metadata":{"id":"Jf_5o-yUTItp"}},{"cell_type":"code","source":["!pwd"],"metadata":{"id":"FG5CSQXCc3Pc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_model():\n","    # Function to load and return neural network model \n","    json_file = open('/content/drive/MyDrive/Final_P_Counting/models/Model_AB_1.json', 'r')\n","    loaded_model_json = json_file.read()\n","    json_file.close()\n","    loaded_model = model_from_json(loaded_model_json)\n","    loaded_model.load_weights(\"/content/drive/MyDrive/Final_P_Counting/weights/model_AB_weights_1.h5\")\n","    return loaded_model\n","\n","def create_img(path):\n","    #Function to load,normalize and return image \n","    print(path)\n","    im = Image.open(path).convert('RGB')\n","    \n","    im = np.array(im)\n","    \n","    im = im/255.0\n","    \n","    im[:,:,0]=(im[:,:,0]-0.485)/0.229\n","    im[:,:,1]=(im[:,:,1]-0.456)/0.224\n","    im[:,:,2]=(im[:,:,2]-0.406)/0.225\n","\n","\n","    im = np.expand_dims(im,axis  = 0)\n","    return im"],"metadata":{"id":"Pz5wjGmuTKPY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(path):\n","    #Function to load image,predict heat map, generate count and return (count , image , heat map)\n","    model = load_model()\n","    image = create_img(path)\n","    ans = model.predict(image)\n","    count = np.sum(ans)\n","    return count,image, ans"],"metadata":{"id":"w1JIpN_6TRjk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 검증"],"metadata":{"id":"kRxwF-YMTlUr"}},{"cell_type":"code","source":["ans, img, hmap = predict('/content/drive/MyDrive/Final_P_Counting/data/part_A_final/test_data/images/IMG_123.jpg')"],"metadata":{"id":"faE1FLEKTTWn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('예측 인원 : %d' %ans,'명')\n","#Print count, image, heat map\n","plt.imshow(img.reshape(img.shape[1],img.shape[2],img.shape[3]))\n","plt.show()\n","plt.imshow(hmap.reshape(hmap.shape[1],hmap.shape[2]) , cmap = c.jet )\n","plt.show()"],"metadata":{"id":"r5OFtcm9TZbh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 정답"],"metadata":{"id":"GXp19UfqTgxW"}},{"cell_type":"code","source":["temp = h5py.File('/content/drive/MyDrive/Final_P_Counting/data/part_A_final/test_data/ground/IMG_162.h5' , 'r')\n","temp_1 = np.asarray(temp['density'])\n","#plt.imshow(temp_1,cmap = c.jet)\n","print(\"Original Count : \",int(np.sum(temp_1)) + 1)"],"metadata":{"id":"20efHQ8VTaQI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Analysis"],"metadata":{"id":"2NxWtWAerULb"}},{"cell_type":"code","source":["def load_model():\n","    \n","    json_file = open('/content/drive/MyDrive/Final_P_Counting/models/Model.json', 'r')\n","    loaded_model_json = json_file.read()\n","    json_file.close()\n","    loaded_model = model_from_json(loaded_model_json)\n","    loaded_model.load_weights(\"/content/drive/MyDrive/Final_P_Counting/weights/model_A_weights.h5\")\n","    return loaded_model\n","\n","\n","def create_img(path):\n","    im = Image.open(path).convert('RGB')\n","    \n","    im = np.array(im)\n","    \n","    im = im/255.0\n","    \n","    im[:,:,0]=(im[:,:,0]-0.485)/0.229\n","    im[:,:,1]=(im[:,:,1]-0.456)/0.224\n","    im[:,:,2]=(im[:,:,2]-0.406)/0.225\n","\n","\n","    im = np.expand_dims(im,axis  = 0)\n","    return im"],"metadata":{"id":"KpT2OgB0rYXu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["root = '/content/drive/MyDrive/Final_P_Counting/data'\n","\n","part_A_test = os.path.join(root,'part_A_final/test_data','images')\n","part_B_test = os.path.join(root,'part_B_final/test_data','images')\n","path_sets_A = [part_A_test]\n","path_sets_B = [part_B_test]"],"metadata":{"id":"T-t19nHOBbGX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## A_test 분석 결과 저장"],"metadata":{"id":"7metCmPDc8ZW"}},{"cell_type":"code","source":["img_paths = []\n","for path in path_sets_A:\n","    for img_path in glob.glob(os.path.join(path, '*.jpg')):\n","        img_paths.append(img_path)\n","print(len(img_paths))"],"metadata":{"id":"IwGgHI6udMna"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = load_model()\n","name = []\n","y_true = []\n","y_pred = []\n","\n","\n","for image in img_paths:\n","    name.append(image)\n","    gt = h5py.File(image.replace('.jpg','.h5').replace('images','ground') )\n","    groundtruth = np.asarray(gt['density'])\n","    num1 = np.sum(groundtruth)\n","    y_true.append(np.sum(num1))\n","    img = create_img(image)\n","    num = np.sum(model.predict(img))\n","    y_pred.append(np.sum(num))\n","\n","    \n","data = pd.DataFrame({'name': name,'y_pred': y_pred,'y_true': y_true})\n","data.to_csv('/content/drive/MyDrive/Final_P_Counting/csv_1/A_on_A_test.csv', sep=',')"],"metadata":{"id":"FE3X5YH_dXzL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## B_test 분석 결과 저장"],"metadata":{"id":"JnIoHGXPdHe8"}},{"cell_type":"code","source":["img_paths = []\n","for path in path_sets:\n","    for img_path in glob.glob(os.path.join(path, '*.jpg')):\n","        img_paths.append(img_path)\n","print(len(img_paths))"],"metadata":{"id":"S4R_nz9GBksO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = load_model()\n","name = []\n","y_true = []\n","y_pred = []\n","\n","\n","for image in img_paths:\n","    name.append(image)\n","    gt = h5py.File(image.replace('.jpg','.h5').replace('images','ground') )\n","    groundtruth = np.asarray(gt['density'])\n","    num1 = np.sum(groundtruth)\n","    y_true.append(np.sum(num1))\n","    img = create_img(image)\n","    num = np.sum(model.predict(img))\n","    y_pred.append(np.sum(num))\n","\n","    \n","data = pd.DataFrame({'name': name,'y_pred': y_pred,'y_true': y_true})\n","data.to_csv('/content/drive/MyDrive/Final_P_Counting/csv_1/B_on_B_test.csv', sep=',')"],"metadata":{"id":"U4G2jr7KBmVj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 오차 확인"],"metadata":{"id":"sysi4uqtcyvt"}},{"cell_type":"code","source":["data = pd.read_csv('/content/drive/MyDrive/Final_P_Counting/csv_1/A_on_A_test.csv')\n","y_true = data['y_true']\n","y_pred = data['y_pred']\n","\n","ans = mean_absolute_error(np.array(y_true),np.array(y_pred))"],"metadata":{"id":"XhyB3ZIdBnLQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"MAE : \" , ans )"],"metadata":{"id":"yr-bkDA3Bqej"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv('/content/drive/MyDrive/Final_P_Counting/csv_1/B_on_B_test.csv')\n","y_true = data['y_true']\n","y_pred = data['y_pred']\n","\n","ans = mean_absolute_error(np.array(y_true),np.array(y_pred))"],"metadata":{"id":"5C0PtuE1BswT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"MAE : \" , ans )"],"metadata":{"id":"nY5hF6GPBx9s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"pf58gjmbOqS5"},"execution_count":null,"outputs":[]}]}